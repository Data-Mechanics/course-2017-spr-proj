# Find the best living area in Boston and corresponding analysis

 *by Minteng Xie, Yue Lei, Zhi Dou*

## 1. Introduction
The City of Boston has performed a significant effort on collecting data over different services and other public information. The diversity of these publicly available datasets allows us to explore various areas in Boston. Particularly, in this project we are focusing on building a website to help people find the best living area in Boston and provide correspnding analysis to their choices.

Since it is a really complex problem to define which area of boston is most suitable for living, we simplify it by four main factors: `Rent`, `Transport`, `Food`, `Safety` and map them to ratings in 1~5. Through our website, users could customize the ratings of these four aspects based on their personal requirements so that they could find the idea place of living.

Besides, interative statistical analysis is provided. Users could either view the overall analysis of four aspects based on great boston area or get a 4-year-long detailed crime analsis graph based on their possible targets, which could help them make better decisions. 

## 2. Datasets

- [Average Rent](http://datamechanics.io/data/minteng_zhidou/rent.txt)
- [MBTA Stops](http://datamechanics.io/data/minteng_zhidou/stops.txt)
- [Active Food Establishment Licenses](https://data.cityofboston.gov/Permitting/Active-Food-Establishment-Licenses/gb6y-34cq)
- [Crime Incidents In Boston(2012-2015)](https://data.cityofboston.gov/Public-Safety/Crime-Incident-Reports-July-2012-August-2015-Sourc/7cdf-6fgx)
- [Crime Incidents In Boston(2015-2017)](https://data.cityofboston.gov/Public-Safety/Crime-Incident-Reports-August-2015-To-Date-Source-/fqn4-4qap)

## 3. Preprocessing
The preprocessing steps were performed based on relational data and map-reduce paradigm.
1. Combine rent data with zipcode of accorsponding area. To achieve that goal, we fetch longitude and latitude based on the name of area in rent dataset via google maps API and then using this location information as input to fetch zipcode also with the help of google maps api. Then we combine location information with rent data and implement aggregation to get the final data set with rent and the zipcode. The data looks like as below:
```json
{ 
    "avg_rent" : 2359, 
    "area" : "Allston", 
    "postal_code" : "02134" }
```
2. Projecting MBTA, Food and Safety data, besides the needed infomation, for the value of key "location", we add tags such that (location, "transport") for MBTA data, (location, "food") for Food data and (location, "crime") for Safety data. Then we create a union of three datasets into the second new dataset. After union operation, selection is used to to remove data with invalid locations (for example, with longitude and latitude equal to 0).
```json
{   
    "address" : "1159 Washington", 
    "location" : [ 42.272239, -71.068856 ], 
    "type" : "food", 
    "zip" : "02126", 
    "businessname" : "SPUKIES PIZZA RESTAURANT", 
    "city" : "Mattapan" }
```

## 4. Methodologies 

### Optimization
Given all the licensed restaurants/ crime incidents/ MBTA stops/ rent price in boston area, We gonna find the best living area with maximizing **`#restaurant`**, **`#MBTA stops`** and minimize **`#crime incidents`** and **`rent price`**. 

We use googlemaps api to find the left bottom/ right top coordinates of boston area. With these coordinates, we could build a big rectangle containing boston area. Then we separate this rectangle into 10 x 10 grids(user could set this scale manually). 

![boston_grid](http://datamechanics.io/data/minteng_zhidou/Boston_grid.png)

Removing those blank grids which don't belong to boston area, we have 52 grids left. Each grid represents a possible target area which contains a potential place for living. Therefore, we could count the number of restaurant/ crime incidents/ MBTA stops(including buses and subway) in every grid and evaluate these numbers by mapping them into scores from 1-5(safety score is reversed by crime). And according to the center coordinate of this grid, googlemaps api could help us to find it belongs to which area(like Allston/ Back Bay/ Fenway/...). 

![box_count](http://datamechanics.io/data/minteng_zhidou/map_with_label.png) 

*(This graph was generated by Boston_Grid_Count.ipynb)*

Then our database would search and find matched rent price, which also should be mapped into reversed scores from 1-5. Then we get the tuple of ratings(The higher the better) for each grid in the format of 
```
(transport, food, safety, rent) 
```
All these data would be stored in a new collection named ```box_count``` as follow.
```json
{   "_id" : 1, 
    "avg_rent" : 1597, 
    "box" : [ [ 42.22788, -71.1642177 ], [ 42.2450451, -71.1373224 ] ],
    "postal_code" : "02136", 
    "grade" : { "transport" : 1, "food" : 1, "safety" : 4, "rent" : 3 }, 
    "area" : "Hyde Park", 
    "count" : { "transport" : 15, "food" : 14, "crime" : 96 } }
```
User could customize rating due to their preference in website. It would search user's ratings requirement in database. If it finds results with every rating of `(transport, food, safety, rent)` above requirement, return the result with maximal sum of these four ratings. Else it would return the result with minimal distance from the requirement ratings. Detailed algorithm could be found under `web/optimization_algorithm.py`

**(need to add some screenshots of website)**

### Statistical Analysis (need to review and add more on new parts of analysis on website)
After finding ideal area for a new company, we would like to dig deeper into those areas, because this area might be the best choice for now, but it might change, with the variation of rental, crime and transportations. So based on current data, we want to study on the trend of these factors, and for now we mainly focus on crime in different blocks(grids). A new dataset [Safety(Crime 2012-2015)](https://data.cityofboston.gov/Public-Safety/Crime-Incident-Reports-July-2012-August-2015-Sourc/7cdf-6fgx) has been added.

Now let *X<sub>ij</sub>* as the the number of crimes happens in block *i* in year *j*. If *X<sub>ij</sub>* and *X<sub>i(j + 1)</sub>* are highly correlated, then we could claim the number of crimes of these two year in this block have similar distribution. Thus if these random variables continuously related to each other, then we could use such correlation to predict the trend of the criminal events in this year.

For each block(grid), we could count the number of crime incidents of each month in different years(2013- 2016). We build a matrix with **```year```** x  **```#blocks```** x **```#month```**(5 x 52 x 12). Then we calculate **correlation coefficient** and **p-value** for each block so that we could compare them in consistent years. We find that for some blocks, these **correlation coefficient** are close to 0 and **p-value** are close to 1, which means for these blocks, their crime incidents in consistent years are not related. However, we have the ability to do reasonable prediction for those blocks which have high **correlation coefficient** and low **p-value** year by year. 

For example, block 27```[[42.34893792999999, -71.0586156], [42.36623191999999, -71.0144498]]```(02109) has a good performance as follow: 

|     year      | correlation coefficient |       p value       |
|:-------------:|:-----------------------:|:-------------------:|
|   2012-2013   |  0.764868546433         |  0.000643432484115  |
|   2013-2014   |  0.836472540464         |  0.003345903249322  |
|   2014-2015   |  0.735134468067         |  0.006447586595446  |
|   2015-2016   |  0.882368732529         |  0.001234912479238  |

From the graph below we could see the trend of block 27 in the consistent four years(2013-2016) are in similar mode.

![block27](http://datamechanics.io/data/minteng_zhidou/Block27.png) 

*(This graph is generated by fitting.ipynb)*

Because this data of these four year have strong linear relationship, there is a high probability that their criminal records have the same tendency through the entire year. Therefore, we fit all the data in four year to find a common pattern of block 27. Now assume, 2016-2017 could still maintain such strong linear relationship, and then we could use this fitting funtion to simulate the trend of crime of block 27 in this year.

![fitting](http://datamechanics.io/data/minteng_zhidou/fitting.png) 

*(This graph is generated by fitting.ipynb)*



## 5. Results (need to add more)
We use Flask and MongoDB to implement the web service. The homepage:
![homepage](http://datamechanics.io/data/minteng_zhidou/web_pages/1 home.png) 

The first new feature/component is to visualize the optimization problem in project2. After we get the top fitted location results, we use Leaflet to show them in an interactive map, with labeled blocks filled in different colors. Also, we implement and show the crime analysis for those location results, the user could get the crime ratio of certain block in the total crime number for different month/year.

The second new feature/component is statistical analysis. We randomly choose 30 blocks and use D3 to show four grades for each block in a bar chart. Next, we compute the correlation coefficient and p-value between four attributes, we we visualize their relationship by setting four attributes as the nodes and the value of (1-abs(Correlation Coefficient))*500 as the edge length, which means that if two attributes have higher Correlation Coefficient, they will be much closer than others. The last plot is to ……..(project 2)

## 6. Future Work

## Reference (need to add more)
1. http://flask.pocoo.org/docs/0.12/
2. 


# Instructions

All scripts and files are new folder ```minteng_tigerlei_zhidou```. All ```.ipynb``` files are just used to plot and show graphs in case of running error inspected by ```initial.py```.

### auth.json
This project use app token from ```boston data portal``` and ```googlemaps geocoding API```. To retrieve data automatically, app token should be added into `auth.json` file as follow format:
```
{
    "services": {
        "googlemapsportal": {
            "service": "https://developers.google.com/maps/",
            "username": "alice_bob@example.org",
            "key": "xxXxXXXXXXxxXXXxXXXXXXxxXxxxxXXxXxxX"
        },
        "cityofbostondataportal": {
            "service": "https://data.cityofboston.gov/",
            "username": "alice_bob@example.org",
            "token": "XxXXXXxXxXxXxxXXXXxxXxXxX",
            "key": "xxXxXXXXXXxxXXXxXXXXXXxxXxxxxXXxXxxX"
        }
    }
}
```

### Trial mode
This project provide a trial mode to complete data retrieve and transformations very quickly (in at most a few seconds) by operating on a very small portion of the input data set(s). All retreieve requests and transformations are limited to 500 records.

To run trial mode on all files:
```python
python3 initial.py --trial
```

To run trial mode on seperate files, remember to uncomment last few lines of the file
```python
if 'trial' in sys.argv:
     <filename>.execute(True)
else:
     <filename>.execute()
```
Then run it with :
```python
python3 <filename>.py --trial
```

### Prepare Database
First, make sure `mongod` process is running on your machine. Then run the following command:
```python
python3 initiate.py minteng_tigerlei_zhidou
```

### Start Web Server
Locate to target folder with:
```shell
cd minteng_tigerlei_zhidou/web
```
Then run the following command:
```python
python3 web.py
```
To view the website, open a browser on your machine and type `127.0.0.1:5000` in address bar.

### Provenance Information
All provenance information could be seen in ```provenance.html``` after running:
```python
python3 initiate.py minteng_tigerlei_zhidou
```

